{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Vimarsh\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import talib as ta\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "data = pd.read_csv('xnas-itch-20230703.tbbo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ts_recv             ts_event  rtype  publisher_id  \\\n",
      "0  1688371200660869841  1688371200660704717      1             2   \n",
      "1  1688371201201402566  1688371201201237816      1             2   \n",
      "2  1688371201233688992  1688371201233524761      1             2   \n",
      "3  1688371201317556361  1688371201317392163      1             2   \n",
      "4  1688371201478520666  1688371201478356044      1             2   \n",
      "\n",
      "   instrument_id action side  depth         price  size  flags  ts_in_delta  \\\n",
      "0             32      T    B      0  194120000000     1    130       165124   \n",
      "1             32      T    B      0  194110000000     2    130       164750   \n",
      "2             32      T    B      0  194110000000     8    130       164231   \n",
      "3             32      T    B      0  194110000000     2    130       164198   \n",
      "4             32      T    B      0  194000000000     7    130       164622   \n",
      "\n",
      "   sequence     bid_px_00     ask_px_00  bid_sz_00  ask_sz_00  bid_ct_00  \\\n",
      "0    303634  193630000000  194120000000         27         27          1   \n",
      "1    304724  193900000000  194110000000          5        400          1   \n",
      "2    304850  193900000000  194110000000          5        398          1   \n",
      "3    305101  193900000000  194110000000          5        390          1   \n",
      "4    306430  193900000000  194000000000          5        200          1   \n",
      "\n",
      "   ask_ct_00 symbol  \n",
      "0          1   AAPL  \n",
      "1          1   AAPL  \n",
      "2          1   AAPL  \n",
      "3          1   AAPL  \n",
      "4          1   AAPL  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TechnicalIndicators:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def add_momentum_indicators(self):\n",
    "        self.data['RSI'] = ta.RSI(self.data['Close'], timeperiod=14)\n",
    "        self.data['MACD'], self.data['MACD_signal'], self.data['MACD_hist'] = ta.MACD(self.data['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "        self.data['Stoch_k'], self.data['Stoch_d'] = ta.STOCH(self.data['High'], self.data['Low'], self.data['Close'],\n",
    "                                                              fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "\n",
    "    def add_volume_indicators(self):\n",
    "        self.data['OBV'] = ta.OBV(self.data['Close'], self.data['Volume'])\n",
    "\n",
    "    def add_volatility_indicators(self):\n",
    "        self.data['Upper_BB'], self.data['Middle_BB'], self.data['Lower_BB'] = ta.BBANDS(self.data['Close'], timeperiod=20)\n",
    "        self.data['ATR_1'] = ta.ATR(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=1)\n",
    "        self.data['ATR_2'] = ta.ATR(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=2)\n",
    "        self.data['ATR_5'] = ta.ATR(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=5)\n",
    "        self.data['ATR_10'] = ta.ATR(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=10)\n",
    "        self.data['ATR_20'] = ta.ATR(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=20)\n",
    "\n",
    "    def add_trend_indicators(self):\n",
    "        self.data['ADX'] = ta.ADX(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=14)\n",
    "        self.data['+DI'] = ta.PLUS_DI(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=14)\n",
    "        self.data['-DI'] = ta.MINUS_DI(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=14)\n",
    "        self.data['CCI'] = ta.CCI(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=5)\n",
    "\n",
    "    def add_other_indicators(self):\n",
    "        self.data['DLR'] = np.log(self.data['Close'] / self.data['Close'].shift(1))\n",
    "        self.data['TWAP'] = self.data['Close'].expanding().mean()\n",
    "        self.data['VWAP'] = (self.data['Volume'] * (self.data['High'] + self.data['Low']) / 2).cumsum() / self.data['Volume'].cumsum()\n",
    "\n",
    "    def add_all_indicators(self):\n",
    "        self.add_momentum_indicators()\n",
    "        self.add_volume_indicators()\n",
    "        self.add_volatility_indicators()\n",
    "        self.add_trend_indicators()\n",
    "        self.add_other_indicators()\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing to create necessary columns\n",
    "data['price']=data['price']/1e9\n",
    "data['bid_px_00']=data['bid_px_00']/1e9\n",
    "data['ask_px_00']=data['ask_px_00']/1e9\n",
    "\n",
    "data['Close'] = data['price']\n",
    "data['Volume'] = data['size']\n",
    "data['High'] = data[['bid_px_00', 'ask_px_00']].max(axis=1)\n",
    "data['Low'] = data[['bid_px_00', 'ask_px_00']].min(axis=1)\n",
    "data['Open'] = data['Close'].shift(1).fillna(data['Close'])\n",
    "\n",
    "\n",
    "ti = TechnicalIndicators(data)\n",
    "df_with_indicators = ti.add_all_indicators()\n",
    "market_features_df = df_with_indicators[35:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59236 entries, 35 to 59270\n",
      "Data columns (total 47 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   ts_recv        59236 non-null  int64  \n",
      " 1   ts_event       59236 non-null  int64  \n",
      " 2   rtype          59236 non-null  int64  \n",
      " 3   publisher_id   59236 non-null  int64  \n",
      " 4   instrument_id  59236 non-null  int64  \n",
      " 5   action         59236 non-null  object \n",
      " 6   side           59236 non-null  object \n",
      " 7   depth          59236 non-null  int64  \n",
      " 8   price          59236 non-null  float64\n",
      " 9   size           59236 non-null  int64  \n",
      " 10  flags          59236 non-null  int64  \n",
      " 11  ts_in_delta    59236 non-null  int64  \n",
      " 12  sequence       59236 non-null  int64  \n",
      " 13  bid_px_00      59236 non-null  float64\n",
      " 14  ask_px_00      59236 non-null  float64\n",
      " 15  bid_sz_00      59236 non-null  int64  \n",
      " 16  ask_sz_00      59236 non-null  int64  \n",
      " 17  bid_ct_00      59236 non-null  int64  \n",
      " 18  ask_ct_00      59236 non-null  int64  \n",
      " 19  symbol         59236 non-null  object \n",
      " 20  Close          59236 non-null  float64\n",
      " 21  Volume         59236 non-null  int64  \n",
      " 22  High           59236 non-null  float64\n",
      " 23  Low            59236 non-null  float64\n",
      " 24  Open           59236 non-null  float64\n",
      " 25  RSI            59236 non-null  float64\n",
      " 26  MACD           59236 non-null  float64\n",
      " 27  MACD_signal    59236 non-null  float64\n",
      " 28  MACD_hist      59236 non-null  float64\n",
      " 29  Stoch_k        59236 non-null  float64\n",
      " 30  Stoch_d        59236 non-null  float64\n",
      " 31  OBV            59236 non-null  float64\n",
      " 32  Upper_BB       59236 non-null  float64\n",
      " 33  Middle_BB      59236 non-null  float64\n",
      " 34  Lower_BB       59236 non-null  float64\n",
      " 35  ATR_1          59236 non-null  float64\n",
      " 36  ATR_2          59236 non-null  float64\n",
      " 37  ATR_5          59236 non-null  float64\n",
      " 38  ATR_10         59236 non-null  float64\n",
      " 39  ATR_20         59236 non-null  float64\n",
      " 40  ADX            59236 non-null  float64\n",
      " 41  +DI            59236 non-null  float64\n",
      " 42  -DI            59236 non-null  float64\n",
      " 43  CCI            59236 non-null  float64\n",
      " 44  DLR            59236 non-null  float64\n",
      " 45  TWAP           59236 non-null  float64\n",
      " 46  VWAP           59236 non-null  float64\n",
      "dtypes: float64(29), int64(15), object(3)\n",
      "memory usage: 21.2+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "market_features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vimarsh\\AppData\\Local\\Temp\\ipykernel_2092\\2390496433.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  market_features_df['OBV_shifted'] = market_features_df['OBV'].shift(1)\n",
      "C:\\Users\\Vimarsh\\AppData\\Local\\Temp\\ipykernel_2092\\2390496433.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  market_features_df['recommend'] = market_features_df.apply(decide_action, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Compute the shifted values for 'OBV' and any other necessary features\n",
    "market_features_df['OBV_shifted'] = market_features_df['OBV'].shift(1)\n",
    "\n",
    "# Define the function to decide action\n",
    "def decide_action(row):\n",
    "    # Buy Conditions\n",
    "    if (row['RSI'] < 30) or \\\n",
    "       (row['MACD'] > row['MACD_signal']) or \\\n",
    "       ((row['Stoch_k'] < 20) and (row['Stoch_k'] > row['Stoch_d'])) or \\\n",
    "       (row['Close'] < row['Lower_BB']) or \\\n",
    "       ((row['ADX'] > 20) and (row['+DI'] > row['-DI'])):\n",
    "        return 1  # Buy\n",
    "    \n",
    "    # Sell Conditions\n",
    "    elif (row['RSI'] > 70) or \\\n",
    "         (row['MACD'] < row['MACD_signal']) or \\\n",
    "         ((row['Stoch_k'] > 80) and (row['Stoch_k'] < row['Stoch_d'])) or \\\n",
    "         (row['Close'] > row['Upper_BB']) or \\\n",
    "         ((row['ADX'] > 20) and (row['+DI'] < row['-DI'])):\n",
    "        return 2  # Sell\n",
    "    \n",
    "    # Hold Condition\n",
    "    else:\n",
    "        return 0  # Hold\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "market_features_df['recommend'] = market_features_df.apply(decide_action, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (41423, 60, 17) (41423,)\n",
      "Validation data shape:  (8876, 60, 17) (8876,)\n",
      "Test data shape:  (8877, 60, 17) (8877,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Columns to be used as features\n",
    "feature_columns = ['Close', 'Volume', 'RSI', 'MACD', 'MACD_signal', 'MACD_hist', \n",
    "                   'Stoch_k', 'Stoch_d', 'OBV', 'Upper_BB', 'Middle_BB', 'Lower_BB', \n",
    "                   'ATR_1', 'ADX', '+DI', '-DI', 'CCI']\n",
    "\n",
    "# Assume market_features_df is your dataframe and 'recommend' is the target column\n",
    "\n",
    "# Select only the specified feature columns\n",
    "features = market_features_df[feature_columns]\n",
    "target = market_features_df['recommend']\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the features and transform\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Convert scaled features back to a DataFrame\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "\n",
    "# Add the target column back to the scaled features DataFrame\n",
    "scaled_features_df['recommend'] = target.values\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(df, seq_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(df) - seq_length):\n",
    "        sequence = df.iloc[i:i+seq_length].drop(columns=['recommend']).values\n",
    "        label = df.iloc[i+seq_length]['recommend']\n",
    "        sequences.append(sequence)\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Create sequences with a sequence length of 60 (for example)\n",
    "seq_length = 60\n",
    "X, y = create_sequences(scaled_features_df, seq_length)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print shapes of the datasets\n",
    "print(\"Training data shape: \", X_train.shape, y_train.shape)\n",
    "print(\"Validation data shape: \", X_val.shape, y_val.shape)\n",
    "print(\"Test data shape: \", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Vimarsh\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Vimarsh\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 60, 17)]             0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 60, 17)               18193     ['input_1[0][0]',             \n",
      " iHeadAttention)                                                     'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 60, 17)               0         ['multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " add (Add)                   (None, 60, 17)               0         ['input_1[0][0]',             \n",
      "                                                                     'dropout[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 60, 17)               34        ['add[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 60, 128)              2304      ['layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 60, 128)              0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 60, 17)               2193      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 60, 17)               0         ['layer_normalization[0][0]', \n",
      "                                                                     'dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 60, 17)               34        ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 60, 17)               18193     ['layer_normalization_1[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 60, 17)               0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 60, 17)               0         ['layer_normalization_1[0][0]'\n",
      "                                                                    , 'dropout_2[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 60, 17)               34        ['add_2[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 60, 128)              2304      ['layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 60, 128)              0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 60, 17)               2193      ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 60, 17)               0         ['layer_normalization_2[0][0]'\n",
      "                                                                    , 'dense_3[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 60, 17)               34        ['add_3[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, 60, 17)               18193     ['layer_normalization_3[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 60, 17)               0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 60, 17)               0         ['layer_normalization_3[0][0]'\n",
      "                                                                    , 'dropout_4[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, 60, 17)               34        ['add_4[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 60, 128)              2304      ['layer_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 60, 128)              0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 60, 17)               2193      ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 60, 17)               0         ['layer_normalization_4[0][0]'\n",
      "                                                                    , 'dense_5[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, 60, 17)               34        ['add_5[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (Mu  (None, 60, 17)               18193     ['layer_normalization_5[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 60, 17)               0         ['multi_head_attention_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 60, 17)               0         ['layer_normalization_5[0][0]'\n",
      "                                                                    , 'dropout_6[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, 60, 17)               34        ['add_6[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 60, 128)              2304      ['layer_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 60, 128)              0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 60, 17)               2193      ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 60, 17)               0         ['layer_normalization_6[0][0]'\n",
      "                                                                    , 'dense_7[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, 60, 17)               34        ['add_7[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 17)                   0         ['layer_normalization_7[0][0]'\n",
      " GlobalAveragePooling1D)                                            ]                             \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 3)                    54        ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 91086 (355.80 KB)\n",
      "Trainable params: 91086 (355.80 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Add, Dense, Dropout, GlobalAveragePooling1D, Input\n",
    "\n",
    "\n",
    "def transformer_encoder_layer(inputs, head_size, num_heads, ff_dim, dropout_rate):\n",
    "    # Multi-head attention and normalization\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
    "    attn_output = Dropout(dropout_rate)(attn_output)\n",
    "    out1 = Add()([inputs, attn_output])\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(out1)\n",
    "    \n",
    "    # Feed-forward network and normalization\n",
    "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
    "    ffn_output = Add()([out1, ffn_output])\n",
    "    ffn_output = LayerNormalization(epsilon=1e-6)(ffn_output)\n",
    "    \n",
    "    return ffn_output\n",
    "\n",
    "def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout_rate):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder_layer(x, head_size, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(3, activation='softmax')(x)  # Output layer for three classes: buy, hold, sell\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Model configuration\n",
    "input_shape = (60, 17)  # 60 timesteps, 17 features\n",
    "head_size = 64\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_layers = 4\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create the model\n",
    "model = build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "41/41 [==============================] - 154s 4s/step - loss: 0.3108 - accuracy: 0.8791 - val_loss: 0.3101 - val_accuracy: 0.8807\n",
      "Epoch 2/12\n",
      "41/41 [==============================] - 150s 4s/step - loss: 0.3067 - accuracy: 0.8793 - val_loss: 0.3138 - val_accuracy: 0.8783\n",
      "Epoch 3/12\n",
      "41/41 [==============================] - 142s 3s/step - loss: 0.3041 - accuracy: 0.8793 - val_loss: 0.3052 - val_accuracy: 0.8792\n",
      "Epoch 4/12\n",
      "41/41 [==============================] - 138s 3s/step - loss: 0.3006 - accuracy: 0.8794 - val_loss: 0.3008 - val_accuracy: 0.8819\n",
      "Epoch 5/12\n",
      "41/41 [==============================] - 124s 3s/step - loss: 0.2976 - accuracy: 0.8800 - val_loss: 0.2986 - val_accuracy: 0.8805\n",
      "Epoch 6/12\n",
      "41/41 [==============================] - 114s 3s/step - loss: 0.2967 - accuracy: 0.8795 - val_loss: 0.2941 - val_accuracy: 0.8820\n",
      "Epoch 7/12\n",
      "41/41 [==============================] - 113s 3s/step - loss: 0.2928 - accuracy: 0.8792 - val_loss: 0.2929 - val_accuracy: 0.8792\n",
      "Epoch 8/12\n",
      "41/41 [==============================] - 138s 3s/step - loss: 0.2900 - accuracy: 0.8810 - val_loss: 0.2904 - val_accuracy: 0.8815\n",
      "Epoch 9/12\n",
      "41/41 [==============================] - 129s 3s/step - loss: 0.2864 - accuracy: 0.8808 - val_loss: 0.2884 - val_accuracy: 0.8802\n",
      "Epoch 10/12\n",
      "41/41 [==============================] - 113s 3s/step - loss: 0.2829 - accuracy: 0.8800 - val_loss: 0.2827 - val_accuracy: 0.8805\n",
      "Epoch 11/12\n",
      "41/41 [==============================] - 126s 3s/step - loss: 0.2803 - accuracy: 0.8816 - val_loss: 0.2829 - val_accuracy: 0.8795\n",
      "Epoch 12/12\n",
      "41/41 [==============================] - 126s 3s/step - loss: 0.2787 - accuracy: 0.8818 - val_loss: 0.2812 - val_accuracy: 0.8822\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=12, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/278 [==============================] - 11s 39ms/step - loss: 0.2756 - accuracy: 0.8834\n",
      "Test Loss: 0.27560245990753174\n",
      "Test Accuracy: 0.8834065794944763\n"
     ]
    }
   ],
   "source": [
    "model.save('trained_transformer_model.h5')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278/278 [==============================] - 10s 35ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.90      0.98      0.94      7796\n",
      "         2.0       0.56      0.21      0.31      1081\n",
      "\n",
      "    accuracy                           0.88      8877\n",
      "   macro avg       0.73      0.59      0.62      8877\n",
      "weighted avg       0.86      0.88      0.86      8877\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7613  183]\n",
      " [ 852  229]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "print(classification_report(y_test, predicted_classes))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, predicted_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=10,         # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "21/21 [==============================] - 166s 8s/step - loss: 0.2865 - accuracy: 0.8808 - val_loss: 0.2794 - val_accuracy: 0.8831 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "21/21 [==============================] - 162s 8s/step - loss: 0.2723 - accuracy: 0.8829 - val_loss: 0.2772 - val_accuracy: 0.8822 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "21/21 [==============================] - 162s 8s/step - loss: 0.2681 - accuracy: 0.8843 - val_loss: 0.2754 - val_accuracy: 0.8822 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "21/21 [==============================] - 163s 8s/step - loss: 0.2659 - accuracy: 0.8855 - val_loss: 0.2778 - val_accuracy: 0.8805 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "21/21 [==============================] - 163s 8s/step - loss: 0.2661 - accuracy: 0.8853 - val_loss: 0.2739 - val_accuracy: 0.8824 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "21/21 [==============================] - 163s 8s/step - loss: 0.2631 - accuracy: 0.8867 - val_loss: 0.2686 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "21/21 [==============================] - 163s 8s/step - loss: 0.2607 - accuracy: 0.8875 - val_loss: 0.2669 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "21/21 [==============================] - 164s 8s/step - loss: 0.2579 - accuracy: 0.8889 - val_loss: 0.2687 - val_accuracy: 0.8826 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "21/21 [==============================] - 183s 9s/step - loss: 0.2581 - accuracy: 0.8881 - val_loss: 0.2639 - val_accuracy: 0.8860 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "21/21 [==============================] - 130s 6s/step - loss: 0.2540 - accuracy: 0.8889 - val_loss: 0.2702 - val_accuracy: 0.8805 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "21/21 [==============================] - 107s 5s/step - loss: 0.2498 - accuracy: 0.8932 - val_loss: 0.2623 - val_accuracy: 0.8854 - lr: 9.0484e-04\n",
      "Epoch 12/15\n",
      "21/21 [==============================] - 152s 7s/step - loss: 0.2485 - accuracy: 0.8937 - val_loss: 0.2604 - val_accuracy: 0.8873 - lr: 8.1873e-04\n",
      "Epoch 13/15\n",
      "21/21 [==============================] - 168s 8s/step - loss: 0.2467 - accuracy: 0.8943 - val_loss: 0.2607 - val_accuracy: 0.8853 - lr: 7.4082e-04\n",
      "Epoch 14/15\n",
      "21/21 [==============================] - 167s 8s/step - loss: 0.2430 - accuracy: 0.8956 - val_loss: 0.2579 - val_accuracy: 0.8918 - lr: 6.7032e-04\n",
      "Epoch 15/15\n",
      "21/21 [==============================] - 169s 8s/step - loss: 0.2416 - accuracy: 0.8949 - val_loss: 0.2567 - val_accuracy: 0.8871 - lr: 6.0653e-04\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=15,\n",
    "    batch_size=2048,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vimarsh\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('finetuned_transformer_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model from the file\n",
    "model = load_model('finetuned_transformer_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnvironment(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, data, daily_trading_limit, transformer_model_path=None,window_size=60):\n",
    "        super(TradingEnvironment, self).__init__()\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.daily_trading_limit = daily_trading_limit\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Extract state columns\n",
    "        self.state_columns = ['Close', 'Volume', 'RSI', 'MACD', 'MACD_signal', 'MACD_hist', 'Stoch_k', 'Stoch_d',\n",
    "                              'OBV', 'Upper_BB', 'Middle_BB', 'Lower_BB', 'ATR_1', 'ADX', '+DI', '-DI', 'CCI']\n",
    "\n",
    "        # Initialize balance, shares held, and total shares traded\n",
    "        self.balance = 10_000_000.0  # $10 million\n",
    "        self.shares_held = 0\n",
    "        self.total_shares_traded = 0\n",
    "\n",
    "        # Define action space: [Hold, Buy, Sell]\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Define observation space based on state columns\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(len(self.state_columns), 1), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Load the pretrained transformer model if provided\n",
    "        if transformer_model_path:\n",
    "            self.model = tf.keras.models.load_model(transformer_model_path)\n",
    "        else:\n",
    "            self.model = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = 10_000_000.0  # $10 million\n",
    "        self.shares_held = 0\n",
    "        self.total_shares_traded = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.trades = []\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        # Ensure we don't go out of bounds\n",
    "      end = self.current_step + self.window_size\n",
    "      if end > len(self.data):\n",
    "        end = len(self.data)\n",
    "        self.current_step = end - self.window_size\n",
    "\n",
    "      obs = self.data[self.state_columns].iloc[self.current_step:end].values\n",
    "      if obs.shape[0] < self.window_size:\n",
    "        # Pad with zeros or repeat entries if not enough data\n",
    "        padding = np.zeros((self.window_size - obs.shape[0], len(self.state_columns)))\n",
    "        obs = np.vstack((padding, obs))\n",
    "\n",
    "      return obs  # This should now be (60, 17)\n",
    "\n",
    "    def step(self, action=None):\n",
    "        if action is None and self.model:\n",
    "            state = self._next_observation()\n",
    "            state = state.reshape(1, *state.shape)\n",
    "            action = self._predict_action(state)\n",
    "\n",
    "        expected_price = self.data.iloc[self.current_step]['ask_px_00']\n",
    "        actual_price = self.data.iloc[self.current_step]['price']\n",
    "        transaction_time = self.data.iloc[self.current_step]['ts_in_delta']\n",
    "        self._take_action(action)\n",
    "        reward = 0\n",
    "        \n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.current_step = 0\n",
    "        if action != 0:\n",
    "            transaction_cost = self._calculate_transaction_cost(self.data.iloc[self.current_step]['Volume'], 0.3, self.data['Volume'].mean())\n",
    "            reward = self._calculate_reward(expected_price, actual_price, transaction_time, transaction_cost)\n",
    "            self.cumulative_reward += reward\n",
    "            if self.trades:\n",
    "                self.trades[-1]['reward'] = reward\n",
    "                self.trades[-1]['transaction_cost'] = transaction_cost\n",
    "                self.trades[-1]['slippage'] = expected_price - actual_price\n",
    "                self.trades[-1]['time_penalty'] = 100 * transaction_time / 1e9\n",
    "        done = self.current_step == len(self.data) - 1\n",
    "        obs = self._next_observation()\n",
    "        info = {\n",
    "            'step': self.current_step,\n",
    "            'action': action,\n",
    "            'price': actual_price,\n",
    "            'shares': self.trades[-1]['shares'] if self.trades else 0\n",
    "        }\n",
    "        self.current_step += 1\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def _predict_action(self, state):\n",
    "     with tf.device('/CPU:0'):  # Ensuring prediction runs on CPU\n",
    "        output = self.model.predict(state)\n",
    "     if 'recommend' in output:\n",
    "        recommended_action = np.argmax(output['recommend'], axis=1)[0]  # Assumes 'recommend' contains softmax probabilities\n",
    "     else:\n",
    "        recommended_action = np.argmax(output, axis=1)[0]  # Fallback if 'recommend' key is not present\n",
    "     return recommended_action\n",
    "\n",
    "    def _take_action(self, action):\n",
    "        current_price = self.data.iloc[self.current_step]['Close']\n",
    "        current_time = pd.to_datetime(self.data.iloc[self.current_step]['ts_event'])\n",
    "        trade_info = {'step': self.current_step, 'timestamp': current_time, 'action': action, 'price': current_price, 'shares': 0, 'reward': 0, 'transaction_cost': 0, 'slippage': 0, 'time_penalty': 0}\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            shares_bought = (self.balance * np.random.uniform(0.001, 0.005)) // current_price\n",
    "            self.balance -= shares_bought * current_price\n",
    "            self.shares_held += shares_bought\n",
    "            self.total_shares_traded += shares_bought\n",
    "            trade_info['shares'] = shares_bought\n",
    "            if shares_bought > 0:\n",
    "                self.trades.append(trade_info)\n",
    "        elif action == 2:  # Sell\n",
    "            shares_sold = min((self.balance * np.random.uniform(0.001, 0.005)) // current_price, self.shares_held)\n",
    "            self.balance += shares_sold * current_price\n",
    "            self.shares_held -= shares_sold\n",
    "            self.total_shares_traded -= shares_sold\n",
    "            trade_info['shares'] = shares_sold\n",
    "            if shares_sold > 0:\n",
    "                self.trades.append(trade_info)\n",
    "\n",
    "    def _calculate_reward(self, expected_price, actual_price, transaction_time, transaction_cost):\n",
    "        slippage = expected_price - actual_price\n",
    "        time_penalty = 100 * transaction_time / 1e9\n",
    "        reward = - (slippage + time_penalty + transaction_cost)\n",
    "        return reward\n",
    "\n",
    "    def _calculate_transaction_cost(self, volume, volatility, daily_volume):\n",
    "        return volatility * np.sqrt(volume / daily_volume)\n",
    "\n",
    "    def run(self):\n",
    "        self.reset()\n",
    "        for _ in range(len(self.data)):\n",
    "            self.step()\n",
    "        return self.cumulative_reward, self.trades\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(f'Shares held: {self.shares_held}')\n",
    "        print(f'Total shares traded: {self.total_shares_traded}')\n",
    "        print(f'Total portfolio value: {self.balance + self.shares_held * self.data.iloc[self.current_step][\"Close\"]}')\n",
    "        print(f'Cumulative reward: {self.cumulative_reward}')\n",
    "        self.print_trades()\n",
    "\n",
    "    def print_trades(self):\n",
    "        trades_df = pd.DataFrame(self.trades)\n",
    "        trades_df.to_csv('trades_transformer.csv', index=False)\n",
    "        for trade in self.trades:\n",
    "            print(f\"Step: {trade['step']}, Timestamp: {trade['timestamp']}, Action: {trade['action']}, Price: {trade['price']}, Shares: {trade['shares']}, Reward: {trade['reward']}, Transaction Cost: {trade['transaction_cost']}, Slippage: {trade['slippage']}, Time Penalty: {trade['time_penalty']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_trading_limit = 1000\n",
    "\n",
    "ticker = 'AAPL'  # Specify the ticker you want to trade\n",
    "ticker_data = market_features_df[market_features_df['symbol'] == ticker]\n",
    "\n",
    "env = TradingEnvironment(ticker_data, daily_trading_limit,transformer_model_path='finetuned_transformer_model.h5') \n",
    " # Adjust window_size if needed\n",
    "\n",
    " # Reset the environment to get the initial observation\n",
    "obs = env.reset()\n",
    "\n",
    "# Run the environment using only the Transformer model\n",
    "env.reset()  # Reset your environment at the start\n",
    "for action in predicted_classes:\n",
    "    state, reward, done, info = env.step(action)  # Execute action in the environment\n",
    "    if done:\n",
    "        break  # Exit if the environment indicates the episode is finished\n",
    "    env.render()\n",
    "# Print the cumulative reward and trades\n",
    "print(f\"Cumulative Reward: {env.cumulative_reward}\")\n",
    "env.print_trades()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
